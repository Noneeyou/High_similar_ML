import os
import scipy.io as sio
import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from sklearn.metrics.pairwise import cosine_similarity

def mat_to_csv(mat_file, output_dir):
    """
    å°† MAT æ–‡ä»¶è½¬æ¢ä¸ºä¸¤ä¸ª CSV æ–‡ä»¶ï¼š
    1. <åŸæ–‡ä»¶å>_data.csv: æ—¶é—´è½´ä¸ä¿¡å·å€¼ä¸€ä¸€å¯¹åº”
    2. <åŸæ–‡ä»¶å>_metadata.csv: function_record å†…çš„è¯´æ˜ä¿¡æ¯
    
    å‚æ•°:
        mat_file: str, è¾“å…¥çš„ .mat æ–‡ä»¶è·¯å¾„
        output_dir: str, è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    """
    # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(mat_file))[0]
    data_csv = os.path.join(output_dir, f"{base_name}_data.csv")
    meta_csv = os.path.join(output_dir, f"{base_name}_metadata.csv")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for f in [data_csv, meta_csv]:
        if os.path.exists(f):
            choice = input(f"æ–‡ä»¶ {f} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦†ç›–ï¼Ÿ(y/n): ").strip().lower()
            if choice != "y":
                print(f"è·³è¿‡ä¿å­˜ {f}")
                return None, None

    # è¯»å– mat æ–‡ä»¶
    mat_data = sio.loadmat(mat_file)
    signal = mat_data['Signal'][0]  # å–ç¬¬ä¸€ä¸ªä¿¡å·å¯¹è±¡

    # ========== æ•°æ®éƒ¨åˆ† ==========
    x_values = signal['x_values'][0]
    y_values = signal['y_values'][0]

    start_value = float(x_values['start_value'][0][0])
    increment = float(x_values['increment'][0][0])
    number_of_values = int(x_values['number_of_values'][0][0])

    # æ—¶é—´åºåˆ—
    time_values = np.arange(start_value, start_value + number_of_values * increment, increment)
    time_values = time_values[:number_of_values]

    # ä¿¡å·å€¼
    signal_data = y_values['values'][0][0].flatten()

    # ä¿å­˜ data.csv
    df = pd.DataFrame({"Time": time_values, "Signal": signal_data})
    df.to_csv(data_csv, index=False)

    # ========== å…ƒæ•°æ®éƒ¨åˆ† ==========
    function_record = signal['function_record'][0]
    meta_dict = {}
    for name in function_record.dtype.names:
        try:
            val = function_record[name][0][0]
            if isinstance(val, np.ndarray) and val.size == 1:
                val = val.item()
            meta_dict[name] = val
        except Exception as e:
            meta_dict[name] = str(function_record[name])

    meta_df = pd.DataFrame(list(meta_dict.items()), columns=["Field", "Value"])
    meta_df.to_csv(meta_csv, index=False)

    print(f"æ•°æ®å·²ä¿å­˜åˆ°:\n {data_csv}\n {meta_csv}")
    return data_csv, meta_csv

import re
from typing import List

__all__ = ["tdms_to_csv"]  # ä»…æš´éœ²å…¬å…±å‡½æ•°


def _safe_name(s: str) -> str:
    """
    ç§æœ‰ï¼šå°†ç»„å/é€šé“åè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶åç‰‡æ®µ
    - éå­—æ¯æ•°å­—å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    - å»æ‰å‰åå¤šä½™ä¸‹åˆ’çº¿
    """
    return re.sub(r"[^\w\-]+", "_", str(s)).strip("_")


def _build_time_axis_from_props(channel) -> np.ndarray | None:
    """
    ç§æœ‰ï¼šæ ¹æ® TDMS é€šé“å±æ€§å°è¯•æ„é€ æ—¶é—´è½´
    ä¼˜å…ˆä½¿ç”¨ channel.time_track()ï¼›å¦åˆ™æ ¹æ® wf_increment(+æ ·æœ¬æ•°) æ„é€ ç›¸å¯¹æ—¶é—´
    è¿”å›:
        np.ndarray | None
    """
    try:
        tt = channel.time_track()  # nptdms æ–°ç‰ˆæœ¬æä¾›
        if tt is not None:
            return np.asarray(tt)
    except Exception:
        pass

    props = getattr(channel, "properties", {}) or {}
    wf_inc = props.get("wf_increment", None)
    if wf_inc is not None:
        n = props.get("wf_samples", None)
        if n is None:
            try:
                n = len(channel)
            except Exception:
                n = None
        if n is not None:
            return np.arange(n, dtype=float) * float(wf_inc)
    return None


def tdms_to_csv(tdms_file: str, output_dir: str) -> str:
    """
    å°† .tdms æ–‡ä»¶å±•å¼€å¹¶ä¿å­˜åˆ°ä¸€ä¸ª CSV æ–‡ä»¶ä¸­

    å‚æ•°:
        tdms_file (str): è¾“å…¥ .tdms æ–‡ä»¶è·¯å¾„
        output_dir (str): å¯¼å‡º .csv æ–‡ä»¶ä¿å­˜ç›®å½•

    è¿”å›:
        str: æœ€ç»ˆä¿å­˜çš„ CSV æ–‡ä»¶è·¯å¾„

    ä¾èµ–:
        pip install nptdms

    è¯´æ˜:
        - è¾“å‡ºæ–‡ä»¶åä¸è¾“å…¥æ–‡ä»¶ä¸€è‡´ï¼Œä»…åç¼€æ”¹ä¸º .csv
        - CSV ä¸­åŒ…å«: group, channel, time(å¯é€‰), value
    """
    from nptdms import TdmsFile  # å»¶è¿Ÿå¯¼å…¥

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    base_name = os.path.splitext(os.path.basename(tdms_file))[0]
    output_file = os.path.join(output_dir, f"{base_name}.csv")

    tdms = TdmsFile.read(tdms_file)

    rows = []

    # éå†æ‰€æœ‰ç»„ä¸é€šé“
    for group in tdms.groups():
        gname = _safe_name(group.name)
        for channel in group.channels():
            cname = _safe_name(channel.name)
            values = np.asarray(channel[:])

            # ç”Ÿæˆæ—¶é—´è½´
            t = _build_time_axis_from_props(channel)
            if t is not None and len(t) == len(values):
                for time, val in zip(t, values):
                    rows.append({"group": gname, "channel": cname,
                                 "time": time, "value": val})
            else:
                for val in values:
                    rows.append({"group": gname, "channel": cname,
                                 "time": None, "value": val})

    df = pd.DataFrame(rows)
    df.to_csv(output_file, index=False)
    print(f"âœ… å·²ä¿å­˜: {output_file}")
    return output_file


def build_local_temporal_graph(
    csv_path: str,
    save_dir: str,
    num_edges: int = 10,
    label_col: int = None
):
    """
    åŸºäºæ—¶é—´é¡ºåºæ„å»ºå±€éƒ¨æ—¶åºå›¾ã€‚
    æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¸Šä¸‹ç›¸é‚»æ ·æœ¬æ„æˆè¾¹ã€‚
    æ ‡ç­¾åˆ—å¯æŒ‡å®šç´¢å¼•ï¼Œè‹¥ä¸æŒ‡å®šåˆ™é»˜è®¤æœ€åä¸€åˆ—ã€‚
    ğŸš« è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ï¼ˆå¸¸ç”¨äºåºå·/IDï¼‰ï¼Œé¿å…è¯¯å…¥ç‰¹å¾è®¡ç®—ã€‚

    å‚æ•°:
        csv_path (str): è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ã€‚
        save_dir (str): å›¾ç»“æ„æ–‡ä»¶çš„ä¿å­˜æ–‡ä»¶å¤¹ã€‚
        num_edges (int): æ¯ä¸ªèŠ‚ç‚¹çš„è¾¹æ•°ï¼ˆä¸Šä¸‹å¹³å‡åˆ†é…ï¼‰ã€‚
        label_col (int): æ ‡ç­¾åˆ—ç´¢å¼•ï¼ˆé»˜è®¤ None â†’ æœ€åä¸€åˆ—ï¼‰ã€‚
    è¿”å›:
        (nodes_csv, edges_csv, graph_pt): ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å…ƒç»„ã€‚
    """
    os.makedirs(save_dir, exist_ok=True)

    # === è¯»å–æ•°æ® ===
    df = pd.read_csv(csv_path)
    num_nodes = len(df)
    if num_nodes == 0:
        raise ValueError("âŒ è¾“å…¥ CSV æ–‡ä»¶ä¸ºç©ºã€‚")

    # === æ ‡ç­¾åˆ—åˆ¤æ–­ ===
    if label_col is None:
        label_col = df.shape[1] - 1

    # === æå–æ ‡ç­¾åˆ— ===
    y = torch.tensor(df.iloc[:, label_col].values, dtype=torch.long)

    # === æ„é€ ç‰¹å¾åˆ—ï¼ˆå»æ‰é¦–åˆ— + æ ‡ç­¾åˆ—ï¼‰===
    drop_cols = [df.columns[0], df.columns[label_col]] if label_col != 0 else [df.columns[0]]
    df_features = df.drop(columns=drop_cols, errors="ignore")

    # ä¿ç•™æ•°å€¼åˆ—
    df_features = df_features.select_dtypes(include=["float", "int"])
    if df_features.shape[1] == 0:
        raise ValueError("âŒ ç‰¹å¾åˆ—ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥ CSVã€‚")

    # === æ„å»ºè¾¹ ===
    half = num_edges // 2
    edges = []
    for i in range(num_nodes):
        start_up = max(0, i - half)
        end_down = min(num_nodes, i + half + 1)
        up_neighbors = list(range(start_up, i))
        down_neighbors = list(range(i + 1, end_down))

        total_needed = num_edges
        current = len(up_neighbors) + len(down_neighbors)
        if current < total_needed:
            remaining = total_needed - current
            if i + half + 1 >= num_nodes:  # ä¸‹æ–¹ä¸å¤Ÿ
                extra_up = list(range(max(0, start_up - remaining), start_up))
                up_neighbors = extra_up + up_neighbors
            elif i - half < 0:  # ä¸Šæ–¹ä¸å¤Ÿ
                extra_down = list(range(end_down, min(num_nodes, end_down + remaining)))
                down_neighbors += extra_down

        for j in up_neighbors + down_neighbors:
            edges.append((i, j))
            edges.append((j, i))

    # === ä¿å­˜èŠ‚ç‚¹ä¸è¾¹ ===
    nodes_path = os.path.join(save_dir, "nodes.csv")
    edges_path = os.path.join(save_dir, "edges.csv")
    graph_path = os.path.join(save_dir, "graph.pt")

    df_features.to_csv(nodes_path, index=False)
    pd.DataFrame(edges, columns=["source", "target"]).to_csv(edges_path, index=False)

    # === æ„å»º PyG å›¾ç»“æ„ ===
    edge_index = torch.tensor(edges, dtype=torch.long).T
    x = torch.tensor(df_features.values, dtype=torch.float)
    data = Data(x=x, edge_index=edge_index, y=y)

    torch.save(data, graph_path)

    print(f"âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± {num_nodes} ä¸ªèŠ‚ç‚¹ï¼Œ{len(edges)//2} æ¡æ— å‘è¾¹")
    print(f"ğŸ“ nodes.csv: {nodes_path}")
    print(f"ğŸ“ edges.csv: {edges_path}")
    print(f"ğŸ“ graph.pt : {graph_path}")
    print(f"ğŸ§© ç‰¹å¾ç»´åº¦: {x.shape[1]} (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)")

    return nodes_path, edges_path, graph_path


def build_similarity_knn_graph(
    csv_path: str,
    save_dir: str,
    num_edges: int = 10,
    label_col: int = None
):
    """
    åŸºäºæ ·æœ¬é—´ä½™å¼¦ç›¸ä¼¼åº¦ + KNN å»ºå›¾ã€‚
    å¯æŒ‡å®šæ ‡ç­¾åˆ—ç´¢å¼•ï¼›è‹¥ä¸æŒ‡å®šåˆ™é»˜è®¤æœ€åä¸€åˆ—ã€‚
    è¾“å‡ºç»“æ„ä¸ build_local_temporal_graph ä¸€è‡´ã€‚

    å‚æ•°:
        csv_path (str): è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ã€‚
        save_dir (str): å›¾ç»“æ„æ–‡ä»¶çš„ä¿å­˜æ–‡ä»¶å¤¹ã€‚
        num_edges (int): æ¯ä¸ªèŠ‚ç‚¹è¿æ¥çš„é‚»ç‚¹æ•°(KNNæ•°é‡)ã€‚
        label_col (int): æ ‡ç­¾åˆ—ç´¢å¼•ï¼ˆé»˜è®¤ None â†’ æœ€åä¸€åˆ—ï¼‰ã€‚
    è¿”å›:
        (nodes_csv, edges_csv, graph_pt): ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å…ƒç»„ã€‚
    """

    # === è¯»å–æ•°æ® ===
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š å·²è¯»å–æ•°æ®: {df.shape}")

    # === æå–æ ‡ç­¾åˆ— ===
    if label_col is None:
        label_col = df.shape[1] - 1
    y = torch.tensor(df.iloc[:, label_col].values, dtype=torch.long)

    # å¿½ç•¥é¦–åˆ—ï¼ˆåºå·ï¼‰+ æ ‡ç­¾åˆ—
    df_features = df.drop(df.columns[[0, label_col]], axis=1, errors="ignore")
    df_features = df_features.select_dtypes(include=["float", "int"])

    features = df_features.values.astype(np.float32)
    num_nodes = features.shape[0]
    print(f"ğŸ§© ä½¿ç”¨ç‰¹å¾åˆ—æ•°: {features.shape[1]} | ç‰¹å¾åˆ—ç¤ºä¾‹: {list(df_features.columns)[:5]} ...")

    # === è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ ===
    print("âš™ï¸ æ­£åœ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ...")
    sim_matrix = cosine_similarity(features)
    np.fill_diagonal(sim_matrix, -np.inf)

    # === KNN è¾¹æ„å»º ===
    print(f"ğŸ” æ­£åœ¨ä¸ºæ¯ä¸ªèŠ‚ç‚¹é€‰å– {num_edges} ä¸ªæœ€ç›¸ä¼¼é‚»å±…...")
    edges = []
    for i in range(num_nodes):
        topk_idx = np.argpartition(sim_matrix[i], -num_edges)[-num_edges:]
        for j in topk_idx:
            edges.append([i, j])
            edges.append([j, i])

    edges = np.array(edges)
    edge_index = torch.tensor(edges.T, dtype=torch.long)
    x = torch.tensor(features, dtype=torch.float)

    # === æ„é€  PyG å¯¹è±¡ ===
    data = Data(x=x, edge_index=edge_index, y=y)

    # === ä¿å­˜ ===
    os.makedirs(save_dir, exist_ok=True)
    base_name = os.path.splitext(os.path.basename(csv_path))[0]
    nodes_csv = os.path.join(save_dir, f"{base_name}_nodes.csv")
    edges_csv = os.path.join(save_dir, f"{base_name}_edges.csv")
    graph_pt = os.path.join(save_dir, f"{base_name}_graph.pt")

    pd.DataFrame(features).to_csv(nodes_csv, index=False)
    pd.DataFrame(edges, columns=["source", "target"]).to_csv(edges_csv, index=False)
    torch.save(data, graph_pt)

    print(f"âœ… å›¾æ„å»ºå®Œæˆï¼Œå…± {num_nodes} ä¸ªèŠ‚ç‚¹ï¼Œ{len(edges)//2} æ¡æ— å‘è¾¹ã€‚")
    print(f"ğŸ“ èŠ‚ç‚¹æ–‡ä»¶: {nodes_csv}")
    print(f"ğŸ“ è¾¹æ–‡ä»¶:   {edges_csv}")
    print(f"ğŸ“ å›¾æ–‡ä»¶:   {graph_pt}")

    return nodes_csv, edges_csv, graph_pt